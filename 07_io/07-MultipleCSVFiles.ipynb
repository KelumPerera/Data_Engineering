{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f24a78fa-fb7b-4f26-bd78-2883a39f0a89",
            "metadata": {},
            "source": [
                "# CSV files 2: multiple files\n",
                "By the end of this lecture you will be able to:\n",
                "- read multiple CSV files with a glob pattern\n",
                "- read multiple CSV files from a list\n",
                "- read multiple CSV files in lazy mode\n",
                "- automate CSV discovery in sub-directories\n",
                "\n",
                "We import Python's built-in `pathlib` module to work with multiple file paths and create sub-directories"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "b939b3a1-683b-4b50-b684-3ab989daf676",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "polars.config.Config"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "from pathlib import Path\n",
                "\n",
                "import polars as pl\n",
                "\n",
                "pl.Config.set_tbl_rows(6)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0edc655e-ee7e-487b-80d4-fee28463ac3a",
            "metadata": {},
            "source": [
                "We need a dataset with multiple CSV files that share the same scheme for this notebook.\n",
                "\n",
                "We create multiple CSV files from the Titanic dataset in a new directory.\n",
                "\n",
                "We begin by reading in the full CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "7aa380c6-464c-44e7-b627-fb373e0d30db",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "csv_file = \"../data/titanic.csv\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "6ebcfbf4-4aea-47a2-af2f-725a274790ed",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "df = pl.read_csv(csv_file)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b4f47946-e2af-47eb-b2ed-99d6da53677e",
            "metadata": {},
            "source": [
                "We create a new sub-directory in this directory.\n",
                "\n",
                "We use the `mkdir` method of a `Path` object to create this new sub-directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "b023c963-e064-403f-affe-7717b4c70cd1",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "# Path to the new directory\n",
                "csvDirectory = Path(\"data_files/csv/multiple_csv\")\n",
                "# Create the new directory if it doesn't already exist\n",
                "csvDirectory.mkdir(parents=True,exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "44b1c0e3-1dc4-4018-b894-2d5563a5b98a",
            "metadata": {},
            "source": [
                "We split the `DataFrame` and write the two new files to the sub-directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "1b2d9173-0b40-4ec1-863c-ccb059144e2d",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "df[:700].write_csv(csvDirectory / \"train.csv\")\n",
                "df[700:].write_csv(csvDirectory / \"test.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fa575804-c0ab-4630-b11e-7bc6f3c0b6cd",
            "metadata": {},
            "source": [
                "## Reading CSVs in eager mode\n",
                "### Reading multiple files with wildcard patterns\n",
                "\n",
                "We can read multiple CSV files with the same schema using a wildcard `*` pattern"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "9aa006e3-b8e7-4791-a33f-6220d7129bc7",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div><style>\n",
                            ".dataframe > thead > tr > th,\n",
                            ".dataframe > tbody > tr > td {\n",
                            "  text-align: right;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<small>shape: (2, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>PassengerId</th><th>Survived</th><th>Pclass</th><th>Name</th><th>Sex</th><th>Age</th><th>SibSp</th><th>Parch</th><th>Ticket</th><th>Fare</th><th>Cabin</th><th>Embarked</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>701</td><td>1</td><td>1</td><td>&quot;Astor, Mrs. Jo…</td><td>&quot;female&quot;</td><td>18.0</td><td>1</td><td>0</td><td>&quot;PC 17757&quot;</td><td>227.525</td><td>&quot;C62 C64&quot;</td><td>&quot;C&quot;</td></tr><tr><td>702</td><td>1</td><td>1</td><td>&quot;Silverthorne, …</td><td>&quot;male&quot;</td><td>35.0</td><td>0</td><td>0</td><td>&quot;PC 17475&quot;</td><td>26.2875</td><td>&quot;E24&quot;</td><td>&quot;S&quot;</td></tr></tbody></table></div>"
                        ],
                        "text/plain": [
                            "shape: (2, 12)\n",
                            "┌─────────────┬──────────┬────────┬──────────────────┬───┬──────────┬─────────┬─────────┬──────────┐\n",
                            "│ PassengerId ┆ Survived ┆ Pclass ┆ Name             ┆ … ┆ Ticket   ┆ Fare    ┆ Cabin   ┆ Embarked │\n",
                            "│ ---         ┆ ---      ┆ ---    ┆ ---              ┆   ┆ ---      ┆ ---     ┆ ---     ┆ ---      │\n",
                            "│ i64         ┆ i64      ┆ i64    ┆ str              ┆   ┆ str      ┆ f64     ┆ str     ┆ str      │\n",
                            "╞═════════════╪══════════╪════════╪══════════════════╪═══╪══════════╪═════════╪═════════╪══════════╡\n",
                            "│ 701         ┆ 1        ┆ 1      ┆ Astor, Mrs. John ┆ … ┆ PC 17757 ┆ 227.525 ┆ C62 C64 ┆ C        │\n",
                            "│             ┆          ┆        ┆ Jacob (Madelein… ┆   ┆          ┆         ┆         ┆          │\n",
                            "│ 702         ┆ 1        ┆ 1      ┆ Silverthorne,    ┆ … ┆ PC 17475 ┆ 26.2875 ┆ E24     ┆ S        │\n",
                            "│             ┆          ┆        ┆ Mr. Spencer      ┆   ┆          ┆         ┆         ┆          │\n",
                            "│             ┆          ┆        ┆ Victor           ┆   ┆          ┆         ┆         ┆          │\n",
                            "└─────────────┴──────────┴────────┴──────────────────┴───┴──────────┴─────────┴─────────┴──────────┘"
                        ]
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(\n",
                "    pl.read_csv(csvDirectory / \"*.csv\")\n",
                "    .head(2)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2f52cb23-9f1f-4add-81d2-28397585ee60",
            "metadata": {},
            "source": [
                "The files are read in alphabetical order where `test` comes before `train`"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "866d6ba4-5e06-4364-8416-7598dc50ac82",
            "metadata": {},
            "source": [
                "#### What happens when we use the wildcard pattern `*`?\n",
                "When we use the wildcard pattern `*` Polars:\n",
                "- make a list of the files\n",
                "- calls `scan_csv` on each file to make a list of `LazyFrames`\n",
                "- does a vertical concatenation of the `LazyFrames`\n",
                "- calls `collect` to return a `DataFrame`\n",
                "\n",
                "Essentially using `read_csv` with `*` is an automatic version of the lazy mode approach we see below!\n",
                "\n",
                "### What happens if there is a potential optimisation?\n",
                "In the query below we do `read_csv` followed by a `filter`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "8ffe449d-d94e-4390-9f29-0a52f592f5b8",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div><style>\n",
                            ".dataframe > thead > tr > th,\n",
                            ".dataframe > tbody > tr > td {\n",
                            "  text-align: right;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<small>shape: (2, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>PassengerId</th><th>Survived</th><th>Pclass</th><th>Name</th><th>Sex</th><th>Age</th><th>SibSp</th><th>Parch</th><th>Ticket</th><th>Fare</th><th>Cabin</th><th>Embarked</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>701</td><td>1</td><td>1</td><td>&quot;Astor, Mrs. Jo…</td><td>&quot;female&quot;</td><td>18.0</td><td>1</td><td>0</td><td>&quot;PC 17757&quot;</td><td>227.525</td><td>&quot;C62 C64&quot;</td><td>&quot;C&quot;</td></tr><tr><td>702</td><td>1</td><td>1</td><td>&quot;Silverthorne, …</td><td>&quot;male&quot;</td><td>35.0</td><td>0</td><td>0</td><td>&quot;PC 17475&quot;</td><td>26.2875</td><td>&quot;E24&quot;</td><td>&quot;S&quot;</td></tr></tbody></table></div>"
                        ],
                        "text/plain": [
                            "shape: (2, 12)\n",
                            "┌─────────────┬──────────┬────────┬──────────────────┬───┬──────────┬─────────┬─────────┬──────────┐\n",
                            "│ PassengerId ┆ Survived ┆ Pclass ┆ Name             ┆ … ┆ Ticket   ┆ Fare    ┆ Cabin   ┆ Embarked │\n",
                            "│ ---         ┆ ---      ┆ ---    ┆ ---              ┆   ┆ ---      ┆ ---     ┆ ---     ┆ ---      │\n",
                            "│ i64         ┆ i64      ┆ i64    ┆ str              ┆   ┆ str      ┆ f64     ┆ str     ┆ str      │\n",
                            "╞═════════════╪══════════╪════════╪══════════════════╪═══╪══════════╪═════════╪═════════╪══════════╡\n",
                            "│ 701         ┆ 1        ┆ 1      ┆ Astor, Mrs. John ┆ … ┆ PC 17757 ┆ 227.525 ┆ C62 C64 ┆ C        │\n",
                            "│             ┆          ┆        ┆ Jacob (Madelein… ┆   ┆          ┆         ┆         ┆          │\n",
                            "│ 702         ┆ 1        ┆ 1      ┆ Silverthorne,    ┆ … ┆ PC 17475 ┆ 26.2875 ┆ E24     ┆ S        │\n",
                            "│             ┆          ┆        ┆ Mr. Spencer      ┆   ┆          ┆         ┆         ┆          │\n",
                            "│             ┆          ┆        ┆ Victor           ┆   ┆          ┆         ┆         ┆          │\n",
                            "└─────────────┴──────────┴────────┴──────────────────┴───┴──────────┴─────────┴─────────┴──────────┘"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(\n",
                "    pl.read_csv(\"data_files/csv/multiple_csv/*.csv\")\n",
                "    .filter(pl.col(\"Pclass\") == 1)\n",
                "    .head(2)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "efc1e592-82b5-4c88-97c3-e963c047ce62",
            "metadata": {},
            "source": [
                "Note that unlike the lazy version we see below the query optimiser is not used in this query. This means that if we follow `read_csv` with - for example - a `filter` method each CSV is read in full into memory and then the `filter` is applied\n",
                "\n",
                "### Reading from a list of file paths\n",
                "\n",
                "If we have a list of file paths we can also read them manually with `pl.concat`.\n",
                "\n",
                "#### Making a file path generator\n",
                "\n",
                "In this example we call `glob` on the `csvDirectory` `Path` object to make `filePathsGenerator`.\n",
                "\n",
                "The `filePathsGenerator` object is a Python `generator`. We can loop through a generator like a list and to produce the next element. If you are not familar with generators [check out the excellent Whirlwind Tour of Python](https://jakevdp.github.io/WhirlwindTourOfPython/12-generators.html) for an introduction"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "b5d10fcb-cca7-444d-93bd-45f28d8b8371",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "filePathsGenerator = csvDirectory.glob(\"*csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6fff7f95-9e06-41b5-b9bf-2080f66f36d4",
            "metadata": {},
            "source": [
                "#### Iterating through the generator\n",
                "We iterate through the files from the generator. \n",
                "\n",
                "> If you want to re-run this cell you need to re-run the cell above first to reset `filePathsGenerator` to the start of the file paths"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "1e50f536-9fa6-4a94-bd08-acefeb8ca1dd",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div><style>\n",
                            ".dataframe > thead > tr > th,\n",
                            ".dataframe > tbody > tr > td {\n",
                            "  text-align: right;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<small>shape: (3, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>PassengerId</th><th>Survived</th><th>Pclass</th><th>Name</th><th>Sex</th><th>Age</th><th>SibSp</th><th>Parch</th><th>Ticket</th><th>Fare</th><th>Cabin</th><th>Embarked</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>701</td><td>1</td><td>1</td><td>&quot;Astor, Mrs. Jo…</td><td>&quot;female&quot;</td><td>18.0</td><td>1</td><td>0</td><td>&quot;PC 17757&quot;</td><td>227.525</td><td>&quot;C62 C64&quot;</td><td>&quot;C&quot;</td></tr><tr><td>702</td><td>1</td><td>1</td><td>&quot;Silverthorne, …</td><td>&quot;male&quot;</td><td>35.0</td><td>0</td><td>0</td><td>&quot;PC 17475&quot;</td><td>26.2875</td><td>&quot;E24&quot;</td><td>&quot;S&quot;</td></tr><tr><td>703</td><td>0</td><td>3</td><td>&quot;Barbara, Miss.…</td><td>&quot;female&quot;</td><td>18.0</td><td>0</td><td>1</td><td>&quot;2691&quot;</td><td>14.4542</td><td>null</td><td>&quot;C&quot;</td></tr></tbody></table></div>"
                        ],
                        "text/plain": [
                            "shape: (3, 12)\n",
                            "┌─────────────┬──────────┬────────┬──────────────────┬───┬──────────┬─────────┬─────────┬──────────┐\n",
                            "│ PassengerId ┆ Survived ┆ Pclass ┆ Name             ┆ … ┆ Ticket   ┆ Fare    ┆ Cabin   ┆ Embarked │\n",
                            "│ ---         ┆ ---      ┆ ---    ┆ ---              ┆   ┆ ---      ┆ ---     ┆ ---     ┆ ---      │\n",
                            "│ i64         ┆ i64      ┆ i64    ┆ str              ┆   ┆ str      ┆ f64     ┆ str     ┆ str      │\n",
                            "╞═════════════╪══════════╪════════╪══════════════════╪═══╪══════════╪═════════╪═════════╪══════════╡\n",
                            "│ 701         ┆ 1        ┆ 1      ┆ Astor, Mrs. John ┆ … ┆ PC 17757 ┆ 227.525 ┆ C62 C64 ┆ C        │\n",
                            "│             ┆          ┆        ┆ Jacob (Madelein… ┆   ┆          ┆         ┆         ┆          │\n",
                            "│ 702         ┆ 1        ┆ 1      ┆ Silverthorne,    ┆ … ┆ PC 17475 ┆ 26.2875 ┆ E24     ┆ S        │\n",
                            "│             ┆          ┆        ┆ Mr. Spencer      ┆   ┆          ┆         ┆         ┆          │\n",
                            "│             ┆          ┆        ┆ Victor           ┆   ┆          ┆         ┆         ┆          │\n",
                            "│ 703         ┆ 0        ┆ 3      ┆ Barbara, Miss.   ┆ … ┆ 2691     ┆ 14.4542 ┆ null    ┆ C        │\n",
                            "│             ┆          ┆        ┆ Saiide           ┆   ┆          ┆         ┆         ┆          │\n",
                            "└─────────────┴──────────┴────────┴──────────────────┴───┴──────────┴─────────┴─────────┴──────────┘"
                        ]
                    },
                    "execution_count": 9,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(\n",
                "    pl.concat(\n",
                "        [pl.read_csv(csvPath) for csvPath in filePathsGenerator]\n",
                "    )\n",
                "    .head(3)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "838fb917-edd7-49d1-971c-0dc62de8cefa",
            "metadata": {},
            "source": [
                "## Scanning CSVs in lazy mode\n",
                "\n",
                "### Scanning multiple files with a wildcard\n",
                "We can scan multiple CSV files in a directory in lazy mode using a wildcard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "b7befb1c-231a-4efd-a9a4-48449986edfb",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "UNION\n",
                        "  PLAN 0:\n",
                        "    FILTER [(col(\"Age\")) > (50.0)] FROM\n",
                        "\n",
                        "\n",
                        "      Csv SCAN data_files/csv/multiple_csv/test.csv\n",
                        "      PROJECT */12 COLUMNS\n",
                        "  PLAN 1:\n",
                        "    FILTER [(col(\"Age\")) > (50.0)] FROM\n",
                        "\n",
                        "\n",
                        "      Csv SCAN data_files/csv/multiple_csv/train.csv\n",
                        "      PROJECT */12 COLUMNS\n",
                        "END UNION\n"
                    ]
                }
            ],
            "source": [
                "print(\n",
                "    pl.scan_csv(csvDirectory / \"*.csv\")\n",
                "    .filter(pl.col(\"Age\") > 50)\n",
                "    .explain()\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3c3cb226-6b8c-47db-9662-573b8a454bf9",
            "metadata": {},
            "source": [
                "The plan shows us that Polars:\n",
                "- carries out the normal `CSV SCAN` on each file to create a chunk in memory for each file\n",
                "- connects the chunks internally to a single object in `UNION` and\n",
                "\n",
                "We evaluate this plan on all the CSVs with `collect`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "abe14efc-1493-43ba-b3e1-ad3038dba9e9",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "<div><style>\n",
                            ".dataframe > thead > tr > th,\n",
                            ".dataframe > tbody > tr > td {\n",
                            "  text-align: right;\n",
                            "  white-space: pre-wrap;\n",
                            "}\n",
                            "</style>\n",
                            "<small>shape: (64, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>PassengerId</th><th>Survived</th><th>Pclass</th><th>Name</th><th>Sex</th><th>Age</th><th>SibSp</th><th>Parch</th><th>Ticket</th><th>Fare</th><th>Cabin</th><th>Embarked</th></tr><tr><td>i64</td><td>i64</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>i64</td><td>i64</td><td>str</td><td>f64</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>715</td><td>0</td><td>2</td><td>&quot;Greenberg, Mr.…</td><td>&quot;male&quot;</td><td>52.0</td><td>0</td><td>0</td><td>&quot;250647&quot;</td><td>13.0</td><td>null</td><td>&quot;S&quot;</td></tr><tr><td>746</td><td>0</td><td>1</td><td>&quot;Crosby, Capt. …</td><td>&quot;male&quot;</td><td>70.0</td><td>1</td><td>1</td><td>&quot;WE/P 5735&quot;</td><td>71.0</td><td>&quot;B22&quot;</td><td>&quot;S&quot;</td></tr><tr><td>766</td><td>1</td><td>1</td><td>&quot;Hogeboom, Mrs.…</td><td>&quot;female&quot;</td><td>51.0</td><td>1</td><td>0</td><td>&quot;13502&quot;</td><td>77.9583</td><td>&quot;D11&quot;</td><td>&quot;S&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>685</td><td>0</td><td>2</td><td>&quot;Brown, Mr. Tho…</td><td>&quot;male&quot;</td><td>60.0</td><td>1</td><td>1</td><td>&quot;29750&quot;</td><td>39.0</td><td>null</td><td>&quot;S&quot;</td></tr><tr><td>695</td><td>0</td><td>1</td><td>&quot;Weir, Col. Joh…</td><td>&quot;male&quot;</td><td>60.0</td><td>0</td><td>0</td><td>&quot;113800&quot;</td><td>26.55</td><td>null</td><td>&quot;S&quot;</td></tr><tr><td>696</td><td>0</td><td>2</td><td>&quot;Chapman, Mr. C…</td><td>&quot;male&quot;</td><td>52.0</td><td>0</td><td>0</td><td>&quot;248731&quot;</td><td>13.5</td><td>null</td><td>&quot;S&quot;</td></tr></tbody></table></div>"
                        ],
                        "text/plain": [
                            "shape: (64, 12)\n",
                            "┌─────────────┬──────────┬────────┬───────────────────┬───┬───────────┬─────────┬───────┬──────────┐\n",
                            "│ PassengerId ┆ Survived ┆ Pclass ┆ Name              ┆ … ┆ Ticket    ┆ Fare    ┆ Cabin ┆ Embarked │\n",
                            "│ ---         ┆ ---      ┆ ---    ┆ ---               ┆   ┆ ---       ┆ ---     ┆ ---   ┆ ---      │\n",
                            "│ i64         ┆ i64      ┆ i64    ┆ str               ┆   ┆ str       ┆ f64     ┆ str   ┆ str      │\n",
                            "╞═════════════╪══════════╪════════╪═══════════════════╪═══╪═══════════╪═════════╪═══════╪══════════╡\n",
                            "│ 715         ┆ 0        ┆ 2      ┆ Greenberg, Mr.    ┆ … ┆ 250647    ┆ 13.0    ┆ null  ┆ S        │\n",
                            "│             ┆          ┆        ┆ Samuel            ┆   ┆           ┆         ┆       ┆          │\n",
                            "│ 746         ┆ 0        ┆ 1      ┆ Crosby, Capt.     ┆ … ┆ WE/P 5735 ┆ 71.0    ┆ B22   ┆ S        │\n",
                            "│             ┆          ┆        ┆ Edward Gifford    ┆   ┆           ┆         ┆       ┆          │\n",
                            "│ 766         ┆ 1        ┆ 1      ┆ Hogeboom, Mrs.    ┆ … ┆ 13502     ┆ 77.9583 ┆ D11   ┆ S        │\n",
                            "│             ┆          ┆        ┆ John C (Anna      ┆   ┆           ┆         ┆       ┆          │\n",
                            "│             ┆          ┆        ┆ Andr…             ┆   ┆           ┆         ┆       ┆          │\n",
                            "│ …           ┆ …        ┆ …      ┆ …                 ┆ … ┆ …         ┆ …       ┆ …     ┆ …        │\n",
                            "│ 685         ┆ 0        ┆ 2      ┆ Brown, Mr. Thomas ┆ … ┆ 29750     ┆ 39.0    ┆ null  ┆ S        │\n",
                            "│             ┆          ┆        ┆ William Solomo…   ┆   ┆           ┆         ┆       ┆          │\n",
                            "│ 695         ┆ 0        ┆ 1      ┆ Weir, Col. John   ┆ … ┆ 113800    ┆ 26.55   ┆ null  ┆ S        │\n",
                            "│ 696         ┆ 0        ┆ 2      ┆ Chapman, Mr.      ┆ … ┆ 248731    ┆ 13.5    ┆ null  ┆ S        │\n",
                            "│             ┆          ┆        ┆ Charles Henry     ┆   ┆           ┆         ┆       ┆          │\n",
                            "└─────────────┴──────────┴────────┴───────────────────┴───┴───────────┴─────────┴───────┴──────────┘"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "(\n",
                "    pl.scan_csv(csvDirectory / \"*.csv\")\n",
                "    .filter(pl.col(\"Age\") > 50)\n",
                "    .collect()\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1bcf0d72-fd16-4e2f-b924-a452498c0479",
            "metadata": {},
            "source": [
                "## Handling variations in column names\n",
                "We cannot concatenate CSVs that have different column names with `pl.scan_csv`\n",
                "\n",
                "In this example we create write two `DataFrames` with slightly different column names to a new directory"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "da4f29bf-150c-4dc8-95b4-2cbe226e54ce",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "df1 = pl.DataFrame(\n",
                "    {\n",
                "        'int_column':[0,1,2]\n",
                "    }\n",
                ")\n",
                "df2 = pl.DataFrame(\n",
                "    {\n",
                "        'Int_Column':[3,4]\n",
                "    }\n",
                ")\n",
                "# Create a sub-directory to hold the CSV for each DataFrame\n",
                "mismatched_column_names_path = Path('data_files/csv/mismatched_column_names/')\n",
                "if not mismatched_column_names_path.exists():\n",
                "    mismatched_column_names_path.mkdir()\n",
                "# Write the DataFrames to a CSV\n",
                "df1.write_csv(mismatched_column_names_path / 'df1.csv')\n",
                "df2.write_csv(mismatched_column_names_path / 'df2.csv')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ccd9d8f2-1396-4201-822a-ec0b4a20d7b0",
            "metadata": {
                "tags": []
            },
            "source": [
                "If we try to call `pl.scan_csv` with a `*` we get an `Exception`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "f4ee4661-4315-4bed-adbf-1c1c43a2eb8e",
            "metadata": {
                "tags": []
            },
            "outputs": [
                {
                    "ename": "ShapeError",
                    "evalue": "unable to vstack, column names don't match: \"int_column\" and \"Int_Column\"",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mShapeError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m (\n\u001b[1;32m      2\u001b[0m     \u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmismatched_column_names_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdf*.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m )\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polars/utils/deprecation.py:95\u001b[0m, in \u001b[0;36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m     92\u001b[0m     _rename_keyword_argument(\n\u001b[1;32m     93\u001b[0m         old_name, new_name, kwargs, function\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, version\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/polars/lazyframe/frame.py:1695\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, no_optimization, slice_pushdown, comm_subplan_elim, comm_subexpr_elim, streaming)\u001b[0m\n\u001b[1;32m   1683\u001b[0m     comm_subplan_elim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1685\u001b[0m ldf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ldf\u001b[38;5;241m.\u001b[39moptimization_toggle(\n\u001b[1;32m   1686\u001b[0m     type_coercion,\n\u001b[1;32m   1687\u001b[0m     predicate_pushdown,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     streaming,\n\u001b[1;32m   1694\u001b[0m )\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
                        "\u001b[0;31mShapeError\u001b[0m: unable to vstack, column names don't match: \"int_column\" and \"Int_Column\""
                    ]
                }
            ],
            "source": [
                "(\n",
                "    pl.scan_csv(mismatched_column_names_path / 'df*.csv')\n",
                "    .collect()\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f32adc21-3e58-4cc8-a62c-f564fa9e00d8",
            "metadata": {},
            "source": [
                "We handle this using the `with_column_names` argument to modify the column names before we concatenate the data from different files.\n",
                "\n",
                "In this example we specify a function to casts the column names to lower case"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2bb35e77-47e6-43c1-851d-512884051131",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "(ß\n",
                "    pl.scan_csv(\n",
                "        mismatched_column_names_path / 'df*.csv',\n",
                "        with_column_names=lambda cols: [col.lower() for col in cols]\n",
                "    )\n",
                "    .collect()\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b6ce3e50-26f2-4edf-9367-81c70c961b26",
            "metadata": {},
            "source": [
                "### Scanning from a list of file paths in lazy mode\n",
                "We can also create a list of scanned CSV files in lazy mode.\n",
                "\n",
                "We re-define the `filePathsGenerator` in this cell so that we can iterate through it again"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "80613e6f-5d54-4184-acf4-b8732fa3ec31",
            "metadata": {},
            "outputs": [],
            "source": [
                "filePathsGenerator = csvDirectory.glob(\"*csv\")\n",
                "queriesList = [\n",
                "    pl.scan_csv(csvPath) for csvPath in filePathsGenerator\n",
                "]\n",
                "queriesList"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1f51e070-924a-485f-84cf-754a2e22de0c",
            "metadata": {},
            "source": [
                "The `queriesList` is a `list` of `LazyFrames`.\n",
                "\n",
                "Polars can evaluate a `list` of `LazyFrames` with `pl.collect_all`.  The output is a `list` of `DataFrames`"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a2725870-8d05-4c32-832c-f3528440b02f",
            "metadata": {},
            "source": [
                "To return the output as a single `DataFrame` we call:\n",
                "- `pl.collect_all` on the `list` to return a `list` of `DataFrames`\n",
                "- `pl.concat` the combine the `list` of `DataFrames` to a single `DataFrame`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "13ad2956-fac8-4109-baf0-d0e47a7ca7a0",
            "metadata": {},
            "outputs": [],
            "source": [
                "pl.concat(\n",
                "    pl.collect_all(queriesList)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e2164a46-2474-42a4-979d-ac83547c9fd5",
            "metadata": {},
            "source": [
                "When you call `pl.collect_all` Polars runs `pl.collect` on each element in parallel.\n",
                "\n",
                "For large datasets we can use streaming with `streaming = True` in `pl.collect_all`.\n",
                "\n",
                "## Discovering file paths\n",
                "In some cases we want an easy way to find all the CSVs in sub-directories.\n",
                "\n",
                "We can use PyArrow in this case. While using PyArrow isn't necessary in this simple example, it is handy with more complicated directory structures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "b18ac785-feb7-4f60-b852-faeddb6a09a6",
            "metadata": {},
            "outputs": [],
            "source": [
                "import pyarrow.dataset as ds\n",
                "\n",
                "dataset = ds.dataset(\n",
                "    csvDirectory,\n",
                "    format=\"csv\"\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8d0674e9-f801-4d33-8e24-7c006d9042b4",
            "metadata": {},
            "source": [
                "We list the files that PyArrow has found"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "80efdd0f-b9e4-461c-933d-ae46630763f4",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "['data_files/csv/multiple_csv/test.csv',\n",
                            " 'data_files/csv/multiple_csv/train.csv']"
                        ]
                    },
                    "execution_count": 15,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset.files"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dcbd0da5-5dc6-459b-89ee-05367e751f98",
            "metadata": {},
            "source": [
                "We can then read these files in eager mode by:\n",
                "- letting PyArrow turn them into an Arrow table and\n",
                "- creating a Polars `DataFrame` from the Arrow table with zero-copy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2829732c-1c61-49df-a1fd-4ca94caeef7e",
            "metadata": {},
            "outputs": [],
            "source": [
                "(\n",
                "    pl.from_arrow(\n",
                "        dataset.to_table()\n",
                "    )\n",
                "    .head(3)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2e01a0e0-8ddd-47e2-bf88-11c255582c16",
            "metadata": {},
            "source": [
                "With PyArrow we can do manual optimisations such as limit the columns or apply a row filter in the arguments of `to_table`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ea3e9e47-869d-4482-a521-294fe62e0a60",
            "metadata": {},
            "outputs": [],
            "source": [
                "(\n",
                "    pl.from_arrow(\n",
                "        dataset.to_table(\n",
                "            columns=[\"Pclass\",\"Age\"],\n",
                "            filter = ds.field(\"Age\") > 70)\n",
                "    )\n",
                "    .head(3)\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "4c6aa378-5356-4925-885e-854375e1255a",
            "metadata": {},
            "source": [
                "See the PyArrow docs for more info on the `dataset` object: https://arrow.apache.org/docs/python/dataset.html"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2518604-4790-4eb2-8172-3fd0d6a288cc",
            "metadata": {},
            "source": [
                "## So which approach should you use?\n",
                "Each of these approaches will work, but these are my opinions for general cases:\n",
                "- If you want to read all files into memory with no query optimisations use `pl.read_csv`\n",
                "- Use a wildcard if you can specify the files using a wildcard\n",
                "- Use a list if you want more control over which files you read\n",
                "- Use PyArrow if you have a more complicated directory structure"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "baf36629-d239-476a-96a4-ab64bb64fe61",
            "metadata": {},
            "source": [
                "## Exercises\n",
                "In the exercises you will develop your understanding of:\n",
                "- reading multiple CSV files in eager mode\n",
                "- reading multiple CSV files in lazy mode\n",
                "- reading CSVs with PyArrow\n",
                "\n",
                "### Exercise 1\n",
                "The NYC taxi dataset CSV has 1000 rows containing records from different days.\n",
                "\n",
                "### Set-up\n",
                "We transform this CSV into a set of partitioned CSVs in sub-directories. \n",
                "\n",
                "We first set the path to the full CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2adb08d2-e9bd-459c-a069-abb138d03670",
            "metadata": {},
            "outputs": [],
            "source": [
                "nyccsv_file = \"../data/nyc_trip_data_1k.csv\""
            ]
        },
        {
            "cell_type": "markdown",
            "id": "57da09fe-bdb1-464d-941a-733ef7a987ed",
            "metadata": {},
            "source": [
                "We now:\n",
                "- read the CSV\n",
                "- add a column that records the date from the `pickup` datetime\n",
                "- partition the `DataFrame` into a dictionary that maps dates to the `DataFrame` for that date"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af22981b-1e28-4110-af6c-0c54076edb06",
            "metadata": {},
            "outputs": [],
            "source": [
                "dailyDfDict = (\n",
                "    pl.read_csv(nyccsv_file,try_parse_dates=True)\n",
                "    .with_columns(\n",
                "    pl.col(\"pickup\").dt.truncate(\"1d\").dt.strftime(\"%Y-%m-%d\").alias(\"pickup_day\")\n",
                "    )\n",
                "    .partition_by(\"pickup_day\",as_dict=True)\n",
                ")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "abbb985d-08da-439d-bfcb-f1e02b81d2c8",
            "metadata": {},
            "source": [
                "The keys of the `dailyDfDict` are the string dates for each day"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1049dff-5323-49a2-97d8-50051ab78cc9",
            "metadata": {},
            "outputs": [],
            "source": [
                "dailyDfDict.keys()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a84340d4-c23a-4a3d-ac57-0cc49297b0f4",
            "metadata": {},
            "source": [
                "The values for each key is a `DataFrame` for that date"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "32757ce9-a002-42d4-a6cb-96f6bf20ce57",
            "metadata": {},
            "outputs": [],
            "source": [
                "dailyDfDict['2022-01-01'].head(3)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c8e1e13e-55ae-4401-9eff-a00899abad9b",
            "metadata": {},
            "source": [
                "We now create a partitioned directory called `daily_nyc` for the data.\n",
                "\n",
                "The name of each sub-directory is a date.\n",
                "\n",
                "The content of each sub-directory is the CSV for that date"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f06b3134-e608-451e-9506-27f8209f39c5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path to the new directory\n",
                "nycCsvDirectory = Path(\"data_files/csv/daily_nyc\")\n",
                "\n",
                "# Create the new directory if it doesn't already exist\n",
                "nycCsvDirectory.mkdir(parents=True,exist_ok=True)\n",
                "\n",
                "# Loop through each date\n",
                "for day, df in dailyDfDict.items():\n",
                "    # Create a Path object for that date\n",
                "    dailyDirectory = (nycCsvDirectory / day)\n",
                "    # Create the sub-directory for that date\n",
                "    dailyDirectory.mkdir(parents=True,exist_ok=True)\n",
                "    # Write a CSV called daily.csv\n",
                "    df.write_csv(dailyDirectory / \"daily.csv\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1a27b57f-d0aa-466a-9008-7446a138a196",
            "metadata": {},
            "source": [
                "We list the contents of `daily_nyc` to see the sub-directories for each date"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "37428579-5150-4c69-a7dc-046d23817058",
            "metadata": {},
            "outputs": [],
            "source": [
                "ls data_files/csv/daily_nyc/"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e6feadc7-c6d4-4031-82cd-4675ef6ffe2d",
            "metadata": {},
            "source": [
                "We list the contents of one sub-directory to show the CSV"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ff2841ef-7f3d-4531-a03c-72f26514f360",
            "metadata": {},
            "outputs": [],
            "source": [
                "ls data_files/csv/daily_nyc/2022-01-01/"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "74c2dcb7-acf0-449c-8a96-b859357a3834",
            "metadata": {},
            "source": [
                "### Now on to the exercise!\n",
                "\n",
                "Read all the CSV files in eager mode using a path with wildcards for the final directory name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d78f8b7f-4974-4520-a1cb-21c0ba5ebfc1",
            "metadata": {},
            "outputs": [],
            "source": [
                "(\n",
                "    pl.read_csv(\n",
                "        \"data_files/csv/daily_nyc<blank>\n",
                "    )\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e3e2df8f-4ed7-4b14-991e-8b8807ce3867",
            "metadata": {},
            "source": [
                "Read the CSV files in eager mode using:\n",
                "- a `glob` and a `generator`\n",
                "- a concatenation of the list of `DataFrames`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "68962c4d-9b9f-4c9f-a4ff-93fcd25edf32",
            "metadata": {},
            "outputs": [],
            "source": [
                "nycFilePathsGenerator = nycCsvDirectory<blank>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "36ba8a1e-5706-4b0d-91cb-e176c5d99127",
            "metadata": {},
            "source": [
                "Read all the CSV files in lazy mode using a path with wildcards for the final directory name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ccdaff2f-3222-489e-b4c8-5c0e9a18459f",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "30725747-3765-41d4-a69f-7982ef4ba9d3",
            "metadata": {},
            "source": [
                "Read all the CSVs in lazy mode **between 2022-01-01 and 2022-01-09** inclusive\n",
                "\n",
                "- Scan the required `DataFrames` by iterating through the generator\n",
                "- Call `collect_all` to evaluate all the `LazyFrames`\n",
                "- `concat` all the `DataFrames`\n",
                "\n",
                "If you want a hint about filtering the dates expand the cell below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e507db69-c36e-463e-a89d-342813ad79a6",
            "metadata": {
                "jupyter": {
                    "source_hidden": true
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "#Hint: in an `if` statement convert the `csvPath` to string with `csvPath.as_posix()` and check if 2022-01-0\n",
                "# is in the string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "018551e6-b09e-41cd-b9d6-cb44b886f296",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "ce7b7557-abda-48c2-b37d-0a426dfe3d6b",
            "metadata": {},
            "source": [
                "### Exercise 2\n",
                "Create a PyArrow `dataset` object with all the CSVs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "30751d20-e153-4756-8b12-a683ba8035fa",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "b112bc7e-3da6-4f33-8882-77b845a1fa2f",
            "metadata": {},
            "source": [
                "List all the CSV files in the dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "373439fe-82d6-447c-b3c9-aed93dfa4ca3",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "1c9097d7-c087-4504-ab84-4df184f5457d",
            "metadata": {},
            "source": [
                "Read all the files into a Polars `DataFrame`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fa2c8b51-5cc7-47f6-bb18-15815cb4a9ee",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "f65a5073-474c-4f7c-9af7-1af067b90e71",
            "metadata": {},
            "source": [
                "## Solutions\n",
                "\n",
                "### Solution to exercise 1"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eca15b09-1b6f-41ad-8f89-1bdb7fcf2d6f",
            "metadata": {},
            "source": [
                "Read all the CSV files in eager mode using a path with wildcards for the final directory name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5606bc16-7137-463a-a6bc-17bbc3d415da",
            "metadata": {
                "jupyter": {
                    "source_hidden": true
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "pl.read_csv(\"data_files/csv/daily_nyc/*/daily.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8faa5876-af30-4e29-a91a-5418715819fa",
            "metadata": {},
            "source": [
                "Read the CSV files in eager mode using:\n",
                "- a `glob` and a `generator`\n",
                "- a concatenation of the list of `DataFrames`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e66739a-eaa5-4932-af48-1afb80160e1b",
            "metadata": {
                "jupyter": {
                    "source_hidden": true
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "filePathsGenerator = nycCsvDirectory.glob(\"*/*.csv\")\n",
                "(\n",
                "    pl.concat(\n",
                "        [pl.read_csv(csvPath) for csvPath in filePathsGenerator]\n",
                "    )\n",
                ").shape"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f1119bf9-51aa-445d-b5c0-91b9bf911fa5",
            "metadata": {},
            "source": [
                "Read all the CSV files in lazy mode using a path with wildcards for the final directory name"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d7647b52-9047-4eea-89be-f143800157dd",
            "metadata": {
                "jupyter": {
                    "source_hidden": true
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "(\n",
                "    pl.scan_csv(\"data_files/csv/daily_nyc/*/daily.csv\")\n",
                "    .collect()\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "60392580-06e2-4a7e-95d6-4e446c528a96",
            "metadata": {},
            "source": [
                "Read all the CSVs in lazy mode *between 2022-01-01 and 2022-01-09** inclusive\n",
                "\n",
                "- Scan the required `DataFrames` by iterating through the generator\n",
                "- Call `collect_all` to evaluate all the `LazyFrames`\n",
                "- `concat` all the `DataFrames`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b08dfe5-83a3-4197-8c2d-1c222502dbf3",
            "metadata": {},
            "outputs": [],
            "source": [
                "#Hint: in an `if` statement convert the `csvPath` to string with `csvPath.as_posix()` and check if 2022-01-0\n",
                "# is in the string"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1165117c-de0f-4224-ba09-f5bec0f81ee6",
            "metadata": {
                "jupyter": {
                    "source_hidden": true
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "nycFilePathsGenerator = nycCsvDirectory.glob(\"*/daily.csv\")\n",
                "(\n",
                "    pl.concat(\n",
                "        pl.collect_all(\n",
                "            [pl.scan_csv(csvPath) for csvPath in nycFilePathsGenerator if \"2022-01-0\" in csvPath.as_posix()]\n",
                "        )\n",
                "    )\n",
                ").shape"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a00b9673-f0fb-4c62-9d32-e3db404cceeb",
            "metadata": {},
            "source": [
                "### Solution to exercise 2\n",
                "Create a PyArrow `dataset` object with all the CSVs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6606fda8-3ecc-4129-85e4-5a1156d875a5",
            "metadata": {
                "jupyter": {
                    "source_hidden": true
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "dataset = ds.dataset(nycCsvDirectory,format=\"csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c3eca454-eac9-44d1-854d-ad724d2132c8",
            "metadata": {},
            "source": [
                "List all the CSV files in the dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3f0f47f7-971a-4ef6-9bb0-598688e3181e",
            "metadata": {
                "jupyter": {
                    "source_hidden": true
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "dataset.files"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3301b5db-eb8d-4f7f-a8ec-23e8116a0152",
            "metadata": {},
            "source": [
                "Read all the files into a Polars `DataFrame`"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e193abab-2fff-4adf-8c3b-326ddc9f1de6",
            "metadata": {
                "jupyter": {
                    "source_hidden": true
                },
                "tags": []
            },
            "outputs": [],
            "source": [
                "(\n",
                "    pl.from_arrow(\n",
                "        dataset.to_table()\n",
                "    )\n",
                "    .head(3)\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.1"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}